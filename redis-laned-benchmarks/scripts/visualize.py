#!/usr/bin/env python3
"""
JMH Benchmark Visualization

Generates publication-ready charts from JMH JSON results:
1. HOL Impact: Line chart showing dramatic p95 drop (laneCount 1â†’4â†’8â†’16)
2. Strategy Comparison: Grouped bars comparing RoundRobin/ThreadAffinity/LeastUsed
3. Selection Overhead: Bar chart showing pure selection cost (nanoseconds)

Usage:
    python scripts/visualize.py build/reports/jmh/results.json

Output:
    build/reports/jmh/hol_impact.png
    build/reports/jmh/strategy_comparison.png
    build/reports/jmh/selection_overhead.png
"""

import json
import sys
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import seaborn as sns

# Publication-ready style
sns.set_theme(style="whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)
plt.rcParams["font.size"] = 11


def load_results(json_path: str) -> List[Dict]:
    """Load JMH JSON results."""
    with open(json_path) as f:
        return json.load(f)


def extract_percentiles(benchmark: Dict) -> Dict[str, float]:
    """Extract p50/p95/p99 from scorePercentiles."""
    percentiles = benchmark["primaryMetric"]["scorePercentiles"]
    return {
        "p50": percentiles.get("50.0", 0),
        "p95": percentiles.get("95.0", 0),
        "p99": percentiles.get("99.0", 0),
    }


def plot_hol_impact(benchmarks: List[Dict], output_dir: Path):
    """
    Plot HOL Impact: Dramatic latency drop from single-lane to multi-lane.
    
    Shows: laneCount (1/4/8/16) vs p50/p95/p99 latency (milliseconds)
    Key insight: p95 drops from ~100ms to ~10ms (10Ã— improvement)
    """
    hol_data = [
        b for b in benchmarks
        if "HolImpactBenchmark" in b["benchmark"]
    ]
    
    if not hol_data:
        print("âš ï¸  No HolImpactBenchmark results found")
        return
    
    # Group by laneCount
    by_lane = {}
    for b in hol_data:
        lane_count = int(b["params"]["laneCount"])
        percentiles = extract_percentiles(b)
        by_lane[lane_count] = percentiles
    
    # Sort by laneCount
    lane_counts = sorted(by_lane.keys())
    p50_values = [by_lane[lc]["p50"] for lc in lane_counts]
    p95_values = [by_lane[lc]["p95"] for lc in lane_counts]
    p99_values = [by_lane[lc]["p99"] for lc in lane_counts]
    
    # Plot
    fig, ax = plt.subplots(figsize=(12, 7))
    
    ax.plot(lane_counts, p50_values, marker="o", linewidth=2.5, label="p50 (median)", color="#2E86AB")
    ax.plot(lane_counts, p95_values, marker="s", linewidth=2.5, label="p95", color="#A23B72")
    ax.plot(lane_counts, p99_values, marker="^", linewidth=2.5, label="p99", color="#F18F01")
    
    ax.set_xlabel("Lane Count", fontsize=13, fontweight="bold")
    ax.set_ylabel("Latency (ms)", fontsize=13, fontweight="bold")
    ax.set_title("HOL Blocking Impact: Single-Lane vs Multi-Lane\n(Lower is better)", 
                 fontsize=15, fontweight="bold", pad=20)
    
    ax.set_xticks(lane_counts)
    ax.set_xticklabels([f"{lc}" for lc in lane_counts])
    ax.legend(loc="upper right", fontsize=12, frameon=True, shadow=True)
    ax.grid(True, alpha=0.3)
    
    # Annotate key improvement
    if len(lane_counts) >= 2:
        p95_baseline = p95_values[0]
        p95_laned = p95_values[1]
        improvement = (p95_baseline - p95_laned) / p95_baseline * 100
        ax.annotate(
            f"{improvement:.0f}% p95 improvement",
            xy=(lane_counts[1], p95_values[1]),
            xytext=(lane_counts[1] + 1, p95_values[1] + (p95_baseline * 0.2)),
            arrowprops=dict(arrowstyle="->", lw=2, color="#A23B72"),
            fontsize=12,
            fontweight="bold",
            color="#A23B72",
            bbox=dict(boxstyle="round,pad=0.5", facecolor="white", edgecolor="#A23B72", lw=2)
        )
    
    plt.tight_layout()
    output_path = output_dir / "hol_impact.png"
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    print(f"âœ… Generated: {output_path}")
    plt.close()


def plot_strategy_comparison(benchmarks: List[Dict], output_dir: Path):
    """
    Plot Strategy Comparison: RoundRobin vs ThreadAffinity vs LeastUsed.
    
    Shows: Grouped bars (3 strategies Ã— 3 lane counts Ã— p95 latency)
    Key insight: LeastUsed has best p95/p99 (dynamic load balancing)
    """
    strategy_data = [
        b for b in benchmarks
        if "StrategyComparisonBenchmark" in b["benchmark"]
    ]
    
    if not strategy_data:
        print("âš ï¸  No StrategyComparisonBenchmark results found")
        return
    
    # Group by (laneCount, strategy)
    by_lane_strategy = {}
    for b in strategy_data:
        lane_count = int(b["params"]["laneCount"])
        strategy = b["params"]["strategyName"]
        percentiles = extract_percentiles(b)
        
        if lane_count not in by_lane_strategy:
            by_lane_strategy[lane_count] = {}
        by_lane_strategy[lane_count][strategy] = percentiles
    
    # Plot grouped bars
    lane_counts = sorted(by_lane_strategy.keys())
    strategies = ["ROUND_ROBIN", "THREAD_AFFINITY", "LEAST_USED"]
    strategy_labels = ["RoundRobin", "ThreadAffinity", "LeastUsed"]
    colors = ["#2E86AB", "#A23B72", "#F18F01"]
    
    fig, ax = plt.subplots(figsize=(14, 7))
    
    x = range(len(lane_counts))
    width = 0.25
    
    for i, (strategy, label, color) in enumerate(zip(strategies, strategy_labels, colors)):
        p95_values = [
            by_lane_strategy[lc].get(strategy, {}).get("p95", 0)
            for lc in lane_counts
        ]
        offset = (i - 1) * width
        ax.bar([xi + offset for xi in x], p95_values, width, label=label, color=color, alpha=0.9)
    
    ax.set_xlabel("Lane Count", fontsize=13, fontweight="bold")
    ax.set_ylabel("p95 Latency (ms)", fontsize=13, fontweight="bold")
    ax.set_title("Strategy Comparison: p95 Latency Under HOL Blocking\n(Lower is better)", 
                 fontsize=15, fontweight="bold", pad=20)
    
    ax.set_xticks(x)
    ax.set_xticklabels([f"{lc} lanes" for lc in lane_counts])
    ax.legend(loc="upper right", fontsize=12, frameon=True, shadow=True)
    ax.grid(True, axis="y", alpha=0.3)
    
    plt.tight_layout()
    output_path = output_dir / "strategy_comparison.png"
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    print(f"âœ… Generated: {output_path}")
    plt.close()


def plot_selection_overhead(benchmarks: List[Dict], output_dir: Path):
    """
    Plot Selection Overhead: Complete cost (acquisition + PING command).
    
    Shows: Bar chart (RoundRobin/ThreadAffinity/LeastUsed total cost in microseconds)
    Key insight: All strategies ~identical (80-150Î¼s), proves selection overhead negligible
    """
    overhead_data = [
        b for b in benchmarks
        if "SelectionOverheadBenchmark" in b["benchmark"]
    ]
    
    if not overhead_data:
        print("âš ï¸  No SelectionOverheadBenchmark results found")
        return
    
    # Extract method â†’ score (microseconds)
    overheads = {}
    for b in overhead_data:
        method = b["benchmark"].split(".")[-1]  # e.g., "roundRobinOverhead"
        score_us = b["primaryMetric"]["score"] * 1000  # ms â†’ Î¼s
        
        if "baseline" in method:
            overheads["Baseline"] = score_us
        elif "roundRobin" in method:
            overheads["RoundRobin"] = score_us
        elif "threadAffinity" in method:
            overheads["ThreadAffinity"] = score_us
        elif "leastUsed" in method:
            overheads["LeastUsed"] = score_us
    
    if not overheads:
        print("âš ï¸  No overhead data extracted")
        return
    
    # Plot
    fig, ax = plt.subplots(figsize=(10, 6))
    
    labels = list(overheads.keys())
    values = list(overheads.values())
    colors = ["#95A99C", "#2E86AB", "#A23B72", "#F18F01"][:len(labels)]
    
    bars = ax.bar(labels, values, color=colors, alpha=0.9, edgecolor="black", linewidth=1.2)
    
    ax.set_ylabel("Latency (microseconds)", fontsize=13, fontweight="bold")
    ax.set_title("Selection Strategy Overhead\n(Connection acquisition + PING command)", 
                 fontsize=15, fontweight="bold", pad=20)
    ax.grid(True, axis="y", alpha=0.3)
    
    # Annotate values
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, height + max(values) * 0.02,
                f"{height:.1f} Î¼s", ha="center", va="bottom", fontsize=11, fontweight="bold")
    
    # Add annotation showing negligible overhead
    if len(values) >= 2:
        baseline = overheads.get("Baseline", 0)
        max_strategy = max(v for k, v in overheads.items() if k != "Baseline")
        if baseline > 0:
            overhead = max_strategy - baseline
            overhead_pct = (overhead / max_strategy) * 100
            ax.text(
                len(labels) / 2, max(values) * 0.85,
                f"Selection overhead: ~{overhead:.1f}Î¼s ({overhead_pct:.2f}% of total)\nProves negligible cost",
                ha="center", fontsize=11, style="italic",
                bbox=dict(boxstyle="round,pad=0.8", facecolor="yellow", alpha=0.3)
            )
    
    plt.tight_layout()
    output_path = output_dir / "selection_overhead.png"
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    print(f"âœ… Generated: {output_path}")
    plt.close()


def main():
    if len(sys.argv) != 2:
        print("Usage: python visualize.py <path-to-jmh-results.json>")
        sys.exit(1)
    
    json_path = Path(sys.argv[1])
    if not json_path.exists():
        print(f"âŒ File not found: {json_path}")
        sys.exit(1)
    
    output_dir = json_path.parent
    print(f"ğŸ“Š Loading JMH results from: {json_path}")
    
    benchmarks = load_results(str(json_path))
    print(f"âœ… Loaded {len(benchmarks)} benchmark results")
    
    print("\nğŸ“ˆ Generating charts...")
    plot_hol_impact(benchmarks, output_dir)
    plot_strategy_comparison(benchmarks, output_dir)
    plot_selection_overhead(benchmarks, output_dir)
    
    print(f"\nâœ¨ All charts generated in: {output_dir}")
    print(f"\nğŸ’¡ Tip: Open charts in browser or include in README/docs")


if __name__ == "__main__":
    main()
